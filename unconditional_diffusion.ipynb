{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_dir = r\"D:\\Data\\Datasets\\landscape_small\"\n",
    "    model_name = \"test3\"\n",
    "    diffusion_steps = 1000\n",
    "    learn_rate = 1e-4 #1e-4\n",
    "    batch_size = 3\n",
    "    image_size = 64\n",
    "    num_samples = 1  # Number of samples to generate per epoch\n",
    "    sample_freq_epochs = 10  # Frequency with with to generate samples (i.e. every x epochs)\n",
    "    load_from_checkpoint = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image_batch(image_batch, filename):\n",
    "    \"\"\"\n",
    "    Save a batch of image tensors to disk as a single grid image\n",
    "\n",
    "    Parameters:\n",
    "    - image_batch: batch of unnormalized image tensors with shape (batch_size, channels, height, width)\n",
    "    - filename: File path and name to save image without extension \n",
    "    \"\"\"\n",
    "    # Normalize images to [0, 1]\n",
    "    image_batch = torch.clamp(image_batch * 0.5 + 0.5, 0, 1)\n",
    "\n",
    "    # Create image grid and convert to PIL image\n",
    "    image_grid = make_grid(image_batch)\n",
    "    image_grid = np.transpose(image_grid.cpu().numpy(), (1, 2, 0))\n",
    "    pil_image = Image.fromarray((image_grid * 255).astype(np.uint8))\n",
    "\n",
    "    # Split the path into directory and filename\n",
    "    path = f\"{filename}.png\"\n",
    "    directory, filename = os.path.split(path)\n",
    "\n",
    "    # Create the directory if it does not exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Save image to disk\n",
    "    pil_image.save(path)\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename):\n",
    "    # Split the path into directory and filename\n",
    "    path = f\"{filename}.pt\"\n",
    "    directory, filename = os.path.split(path)\n",
    "\n",
    "    # Create the directory if it does not exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Save checkpoint state\n",
    "    torch.save(state, path)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer, diffusion):\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    diffusion.load_state_dict(checkpoint['diffusion'])\n",
    "    return checkpoint['epoch'], checkpoint['loss_history']\n",
    "\n",
    "\n",
    "def get_dataloader(data_dir, batch_size, image_size):\n",
    "    # Define image transformations\n",
    "    # TODO: Review if this can be improved.\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(round(image_size * 5/4)),\n",
    "        transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalize from [0, 1] to [-1, 1]\n",
    "    ])\n",
    "\n",
    "    # Create an instance of the ImageFolder dataset\n",
    "    dataset = ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "    # Create a DataLoader to batch and shuffle the data\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class UNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(UNet, self).__init__()\n",
    "#         self.conv = nn.Conv2d(3, 3, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "#     def forward(self, x, t):\n",
    "#         x = self.conv(x)\n",
    "#         return x\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, beta):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.step = 0\n",
    "\n",
    "    def update_model_average(self, ma_model, current_model):\n",
    "        for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
    "            old_weight, up_weight = ma_params.data, current_params.data\n",
    "            ma_params.data = self.update_average(old_weight, up_weight)\n",
    "\n",
    "    def update_average(self, old, new):\n",
    "        if old is None:\n",
    "            return new\n",
    "        return old * self.beta + (1 - self.beta) * new\n",
    "\n",
    "    def step_ema(self, ema_model, model, step_start_ema=2000):\n",
    "        if self.step < step_start_ema:\n",
    "            self.reset_parameters(ema_model, model)\n",
    "            self.step += 1\n",
    "            return\n",
    "        self.update_model_average(ema_model, model)\n",
    "        self.step += 1\n",
    "\n",
    "    def reset_parameters(self, ema_model, model):\n",
    "        ema_model.load_state_dict(model.state_dict())\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, channels, size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.size = size\n",
    "        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([channels])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([channels]),\n",
    "            nn.Linear(channels, channels),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(channels, channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.channels, self.size * self.size).swapaxes(1, 2)\n",
    "        x_ln = self.ln(x)\n",
    "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
    "        attention_value = attention_value + x\n",
    "        attention_value = self.ff_self(attention_value) + attention_value\n",
    "        return attention_value.swapaxes(2, 1).view(-1, self.channels, self.size, self.size)\n",
    "\n",
    "# Wrapper for 2 convolutional layers\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, mid_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            return F.gelu(x + self.double_conv(x))\n",
    "        else:\n",
    "            return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "        )\n",
    "\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(\n",
    "                emb_dim,\n",
    "                out_channels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x = self.maxpool_conv(x)\n",
    "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "        return x + emb\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.conv = nn.Sequential(\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels, in_channels // 2),\n",
    "        )\n",
    "\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(\n",
    "                emb_dim,\n",
    "                out_channels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip_x, t):\n",
    "        x = self.up(x)\n",
    "        x = torch.cat([skip_x, x], dim=1)\n",
    "        x = self.conv(x)\n",
    "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "        return x + emb\n",
    "\n",
    "# Since this is a simple UNet, seperate modules are not created for the encoder, decoder and bottleneck\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, c_in=3, c_out=3, time_dim=256, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.time_dim = time_dim\n",
    "        self.inc = DoubleConv(c_in, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.sa1 = SelfAttention(128, 32)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.sa2 = SelfAttention(256, 16)\n",
    "        self.down3 = Down(256, 256)\n",
    "        self.sa3 = SelfAttention(256, 8)\n",
    "\n",
    "        self.bot1 = DoubleConv(256, 512)\n",
    "        self.bot2 = DoubleConv(512, 512)\n",
    "        self.bot3 = DoubleConv(512, 256)\n",
    "\n",
    "        self.up1 = Up(512, 128)\n",
    "        self.sa4 = SelfAttention(128, 16)\n",
    "        self.up2 = Up(256, 64)\n",
    "        self.sa5 = SelfAttention(64, 32)\n",
    "        self.up3 = Up(128, 64)\n",
    "        self.sa6 = SelfAttention(64, 64)\n",
    "        self.outc = nn.Conv2d(64, c_out, kernel_size=1)\n",
    "\n",
    "    def pos_encoding(self, t, channels):\n",
    "        inv_freq = 1.0 / (\n",
    "            10000\n",
    "            ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n",
    "        )\n",
    "        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Time is embedded using sinusoidal embedding as is the most common DDPM approach\n",
    "        t = t.unsqueeze(-1).type(torch.float)\n",
    "        t = self.pos_encoding(t, self.time_dim)\n",
    "\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1, t)\n",
    "        x2 = self.sa1(x2)\n",
    "        x3 = self.down2(x2, t)\n",
    "        x3 = self.sa2(x3)\n",
    "        x4 = self.down3(x3, t)\n",
    "        x4 = self.sa3(x4)\n",
    "\n",
    "        x4 = self.bot1(x4)\n",
    "        x4 = self.bot2(x4)\n",
    "        x4 = self.bot3(x4)\n",
    "\n",
    "        x = self.up1(x4, x3, t)\n",
    "        x = self.sa4(x)\n",
    "        x = self.up2(x, x2, t)\n",
    "        x = self.sa5(x)\n",
    "        x = self.up3(x, x1, t)\n",
    "        x = self.sa6(x)\n",
    "        output = self.outc(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion:\n",
    "    def __init__(self, device, diffusion_steps=1000, image_size=64, beta_1=1e-4, beta_T=0.02):\n",
    "        # Initialize class variables\n",
    "        self.diffusion_steps = diffusion_steps\n",
    "        self.device = device\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_T = beta_T\n",
    "\n",
    "        # Compute beta decay schedule\n",
    "        self.compute_beta_schedule()\n",
    "\n",
    "    def compute_beta_schedule(self):\n",
    "        self.beta = torch.linspace(self.beta_1, self.beta_T, self.diffusion_steps, device=self.device)\n",
    "        self.alpha = 1 - self.beta\n",
    "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "    def apply_noise(self, x_0):\n",
    "        \"\"\"\n",
    "        Apply noise to batch of image tensors based on beta decay schedule at a uniformly distributed time step sample.\n",
    "        \"\"\"\n",
    "        # Sample timestep: t ∼ Uniform({1, . . . , T})\n",
    "        t = torch.randint(1, self.diffusion_steps + 1, (x_0.shape[0],), device=self.device)\n",
    "\n",
    "        # Sample noise: epsilon ~ N(0,I)\n",
    "        epsilon = torch.randn_like(x_0, device=self.device)\n",
    "\n",
    "        # Apply noise to image based on beta schedule\n",
    "        sqrt_alpha_bar_t = torch.sqrt(self.alpha_bar[t-1]).view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha_bar_t = torch.sqrt(1 - self.alpha_bar[t-1]).view(-1, 1, 1, 1)\n",
    "        x_t = (sqrt_alpha_bar_t * x_0) + (sqrt_one_minus_alpha_bar_t * epsilon)\n",
    "\n",
    "        return x_t, t, epsilon\n",
    "    \n",
    "    def remove_noise(self, x_t, t, epsilon_pred):\n",
    "        if t > 1:\n",
    "            z = torch.normal(0, 1, x_t.size(), device=Config.device)\n",
    "        else:\n",
    "            z = torch.zeros(x_t.size(), device=Config.device)\n",
    "\n",
    "        inv_sqrt_alpha_t = 1 / torch.sqrt(self.alpha[t-1])\n",
    "        one_minus_alpha_t = 1 - self.alpha[t-1]\n",
    "        sqrt_one_minus_alpha_bar_t = torch.sqrt(1 - self.alpha_bar[t-1])\n",
    "        subtract = (one_minus_alpha_t / sqrt_one_minus_alpha_bar_t) * epsilon_pred\n",
    "        sigma_t = torch.sqrt(self.beta[t-1])\n",
    "\n",
    "        return inv_sqrt_alpha_t * (x_t - subtract) + sigma_t * z\n",
    "        \n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"\n",
    "        Get the current state of the diffusion model for checkpointing.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'diffusion_steps': self.diffusion_steps,\n",
    "            'beta_1': self.beta_1,\n",
    "            'beta_T': self.beta_T,\n",
    "            'alpha_t': self.alpha,\n",
    "            'alpha_bar_t': self.alpha_bar\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state):\n",
    "        \"\"\"\n",
    "        Load the state of the diffusion model from a checkpoint.\n",
    "        \"\"\"\n",
    "        self.diffusion_steps = state['diffusion_steps']\n",
    "        self.beta_1 = state['beta_1']\n",
    "        self.beta_T = state['beta_T']\n",
    "        self.compute_beta_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs, dataloader, checkpoint=None):\n",
    "\n",
    "    # Initialize other training parameters\n",
    "    criterion = nn.MSELoss()\n",
    "    diffusion = Diffusion(device=Config.device)\n",
    "    model = UNet().to(Config.device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=Config.learn_rate)\n",
    "\n",
    "    if checkpoint:\n",
    "        # Initialize parameters for training from last checkpoint\n",
    "        start_epoch, loss_history = load_checkpoint(checkpoint, model, optimizer, diffusion)\n",
    "    else:\n",
    "        # Initialize parameters for training from scratch\n",
    "        start_epoch = 0\n",
    "        loss_history = []\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()  # Set to model to train mode\n",
    "        epoch_loss = 0\n",
    "\n",
    "        # Train model (algorithm 1)\n",
    "        for x_0, _ in tqdm(dataloader, desc=\"Training\"):\n",
    "            x_0 = x_0.to(Config.device)  # Move images to active compute device\n",
    "\n",
    "            x_t, t, epsilon = diffusion.apply_noise(x_0)  # Apply noise to images (forward process)\n",
    "            epsilon_pred = model(x_t, t)  # Predict noise using model (reverse process)\n",
    "            batch_loss = criterion(epsilon_pred, epsilon)  # Compute loss\n",
    "\n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += batch_loss.item()\n",
    "\n",
    "        # Update loss history\n",
    "        loss_history.append(epoch_loss)\n",
    "\n",
    "        # Display epoch and loss\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss/len(dataloader):.4f}\")\n",
    "\n",
    "        # Save model training checkpoint\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'loss_history': loss_history,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'diffusion': diffusion.state_dict()\n",
    "        }, os.path.join(\"models\", Config.model_name, f\"checkpoint\"))\n",
    "\n",
    "        # Sample images from model (algorithm 2)\n",
    "        if (epoch + 1) % Config.sample_freq_epochs == 0:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                sample_shape = (Config.num_samples, *dataloader.dataset[0][0].size())\n",
    "                x_t = torch.normal(0, 1, sample_shape, device=Config.device)  # Initialized to x_T\n",
    "\n",
    "                # Iterate over all reverse diffusion time steps from T to 1\n",
    "                for t in tqdm(range(diffusion.diffusion_steps, 0, -1), desc=\"Sampling\"):\n",
    "                    t_vec = t * torch.ones(x_t.shape[0], device=Config.device)\n",
    "                    epsilon_pred = model(x_t, t_vec)\n",
    "                    x_t_minus_1 = diffusion.remove_noise(x_t, t, epsilon_pred)\n",
    "                    x_t = x_t_minus_1\n",
    "\n",
    "                save_image_batch(x_t, os.path.join(\"results\", Config.model_name, f\"{epoch + 1}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [00:39<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100], Loss: 0.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [00:37<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/100], Loss: 0.0164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [00:38<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/100], Loss: 0.0228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 96/96 [00:38<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100], Loss: 0.0225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███▏      | 30/96 [00:11<00:26,  2.51it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Load the checkpoint\n",
    "if Config.load_from_checkpoint:\n",
    "    checkpoint = torch.load(os.path.join(\"models\", Config.model_name, f\"checkpoint.pt\"))\n",
    "else:\n",
    "    checkpoint = None\n",
    "\n",
    "# Initialize dataloader\n",
    "dataloader = get_dataloader(Config.data_dir, Config.batch_size, Config.image_size)\n",
    "\n",
    "# Train model\n",
    "torch.cuda.empty_cache()\n",
    "train_model(100, dataloader, checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee641",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
