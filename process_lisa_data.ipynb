{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DATA_DIR = 'D:\\Data\\Datasets\\custom_traffic_light'\n",
    "DATA_DIR = 'D:\\Data\\Datasets\\lisa_traffic_light'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sequences\n",
    "sequences = {'dayTrain': ['dayClip1', 'dayClip2', 'dayClip3', 'dayClip4', 'dayClip5', 'dayClip6', 'dayClip7', \n",
    "                          'dayClip8', 'dayClip9', 'dayClip10', 'dayClip11', 'dayClip12', 'dayClip13'],\n",
    "            'daySequence1': ['']}\n",
    "\n",
    "# Load annotations\n",
    "dfs = []\n",
    "for seq in sequences.keys():\n",
    "    for subseq in sequences[seq]:\n",
    "        annotations = os.path.join(DATA_DIR, 'Annotations', 'Annotations', seq, subseq, 'frameAnnotationsBOX.csv')\n",
    "        seq_df = pd.read_csv(annotations, delimiter=';')\n",
    "\n",
    "        # Add 'sequence' and 'subsequence' columns\n",
    "        seq_df['sequence'] = seq\n",
    "        seq_df['subsequence'] = subseq\n",
    "\n",
    "        # Append to the combined DataFrame\n",
    "        dfs.append(seq_df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Compute traffic light width and height columns\n",
    "df['width'] = df['Lower right corner X'] - df['Upper left corner X']\n",
    "df['height'] = df['Lower right corner Y'] - df['Upper left corner Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 13294/13294 [04:04<00:00, 54.27it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Parameters\n",
    "image_size = 80\n",
    "height_thresh = 32\n",
    "scale = image_size / height_thresh\n",
    "\n",
    "# Ensure the directory exists\n",
    "classes = df['Annotation tag'].unique()\n",
    "for cls in classes:\n",
    "    output_dir = os.path.join(OUTPUT_DATA_DIR, cls)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Filter for largest traffic lights and group by filename\n",
    "filtered_df = df[df['height'] >= height_thresh]\n",
    "grouped = filtered_df.groupby('Filename')\n",
    "\n",
    "# Iterate through each group\n",
    "for filepath, group in tqdm(grouped, desc=\"Processing images\"):\n",
    "    # Extract sequence for group. Assumes the sequence will be the same for all rows in the group\n",
    "    seq = group['sequence'].values.tolist()[0]\n",
    "    subseq = group['subsequence'].values.tolist()[0]\n",
    "\n",
    "    # Get image file path\n",
    "    basename = os.path.basename(filepath)\n",
    "    filenum = re.search(r'(\\d+)\\.jpg$', basename).group(1)\n",
    "    filename = os.path.join(DATA_DIR, seq, seq, subseq, 'frames', basename)\n",
    "    \n",
    "    # Get bounding box coordinates and class\n",
    "    bounding_boxes = group[['Upper left corner X', 'Upper left corner Y', 'Lower right corner X', \n",
    "                            'Lower right corner Y']].values.tolist()\n",
    "    classes = group['Annotation tag'].values.tolist()\n",
    "    \n",
    "    img = Image.open(filename)\n",
    "    for i, box in enumerate(bounding_boxes):\n",
    "        # Access columns by name\n",
    "        cls = classes[i]\n",
    "\n",
    "        # Calculating the center of the bounding box\n",
    "        center_x = (box[0] + box[2]) / 2\n",
    "        center_y = (box[1] + box[3]) / 2\n",
    "\n",
    "        # Determining the largest dimension and increase it by 100%\n",
    "        box_width = box[2] - box[0]\n",
    "        box_height = box[3] - box[1]\n",
    "        largest_dimension = max(box_width, box_height) * scale\n",
    "\n",
    "        # Creating a new square bounding box\n",
    "        half_size = largest_dimension / 2\n",
    "        new_box = [\n",
    "            max(center_x - half_size, 0), # left\n",
    "            max(center_y - half_size, 0), # upper\n",
    "            min(center_x + half_size, img.width), # right\n",
    "            min(center_y + half_size, img.height) # lower\n",
    "        ]\n",
    "\n",
    "        # Cropping the image\n",
    "        cropped_img = img.crop(new_box)\n",
    "\n",
    "        # Resizing the image to 64x64\n",
    "        cropped_img = cropped_img.resize((image_size, image_size))\n",
    "\n",
    "        # Constructing the filename using the counter\n",
    "        filename = f\"{seq}_{subseq}_{filenum}_{i}.jpg\"\n",
    "        file_path = os.path.join(OUTPUT_DATA_DIR, cls, filename)\n",
    "\n",
    "        # Save the cropped image\n",
    "        cropped_img.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stop           14061\n",
       "go             11347\n",
       "stopLeft        6595\n",
       "goLeft           686\n",
       "warning          603\n",
       "warningLeft      206\n",
       "Name: Annotation tag, dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df['Annotation tag'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee641",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
