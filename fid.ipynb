{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#The FID implementation is referenced friom https://github.com/hukkelas/pytorch-frechet-inception-distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import inception_v3\n",
    "import cv2\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from scipy import linalg\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartialInceptionNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, transform_input=True):\n",
    "        super().__init__()\n",
    "        self.inception_network = inception_v3(pretrained=True)\n",
    "        self.inception_network.Mixed_7c.register_forward_hook(self.output_hook)\n",
    "        self.transform_input = transform_input\n",
    "\n",
    "    def output_hook(self, module, input, output):\n",
    "        # N x 2048 x 8 x 8\n",
    "        self.mixed_7c_output = output\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: shape (N, 3, 299, 299) dtype: torch.float32 in range 0-1\n",
    "        Returns:\n",
    "            inception activations: torch.tensor, shape: (N, 2048), dtype: torch.float32\n",
    "        \"\"\"\n",
    "        assert x.shape[1:] == (3, 299, 299), \"Expected input shape to be: (N,3,299,299)\" +\\\n",
    "                                             \", but got {}\".format(x.shape)\n",
    "        x = x * 2 -1 # Normalize to [-1, 1]\n",
    "\n",
    "        # Trigger output hook\n",
    "        self.inception_network(x)\n",
    "\n",
    "        # Output: N x 2048 x 1 x 1 \n",
    "        activations = self.mixed_7c_output\n",
    "        activations = torch.nn.functional.adaptive_avg_pool2d(activations, (1,1))\n",
    "        activations = activations.view(x.shape[0], 2048)\n",
    "        return activations\n",
    "\n",
    "\n",
    "def get_activations(images, batch_size):\n",
    "    \"\"\"\n",
    "    Calculates activations for last pool layer for all iamges\n",
    "    --\n",
    "        Images: torch.array shape: (N, 3, 299, 299), dtype: torch.float32\n",
    "        batch size: batch size used for inception network\n",
    "    --\n",
    "    Returns: np array shape: (N, 2048), dtype: np.float32\n",
    "    \"\"\"\n",
    "    assert images.shape[1:] == (3, 299, 299), \"Expected input shape to be: (N,3,299,299)\" +\\\n",
    "                                              \", but got {}\".format(images.shape)\n",
    "\n",
    "    num_images = images.shape[0]\n",
    "    inception_network = PartialInceptionNetwork()\n",
    "    inception_network = inception_network.to(device)\n",
    "    inception_network.eval()\n",
    "    n_batches = int(np.ceil(num_images  / batch_size))\n",
    "    inception_activations = np.zeros((num_images, 2048), dtype=np.float32)\n",
    "    for batch_idx in range(n_batches):\n",
    "        start_idx = batch_size * batch_idx\n",
    "        end_idx = batch_size * (batch_idx + 1)\n",
    "\n",
    "        ims = images[start_idx:end_idx]\n",
    "        ims = ims.to(device)\n",
    "        activations = inception_network(ims)\n",
    "        activations = activations.detach().cpu().numpy()\n",
    "        assert activations.shape == (ims.shape[0], 2048), \"Expexted output shape to be: {}, but was: {}\".format((ims.shape[0], 2048), activations.shape)\n",
    "        inception_activations[start_idx:end_idx, :] = activations\n",
    "    return inception_activations\n",
    "\n",
    "\n",
    "\n",
    "def calculate_activation_statistics(images, batch_size):\n",
    "    \"\"\"Calculates the statistics used by FID\n",
    "    Args:\n",
    "        images: torch.tensor, shape: (N, 3, H, W), dtype: torch.float32 in range 0 - 1\n",
    "        batch_size: batch size to use to calculate inception scores\n",
    "    Returns:\n",
    "        mu:     mean over all activations from the last pool layer of the inception model\n",
    "        sigma:  covariance matrix over all activations from the last pool layer \n",
    "                of the inception model.\n",
    "\n",
    "    \"\"\"\n",
    "    act = get_activations(images, batch_size)\n",
    "    mu = np.mean(act, axis=0)\n",
    "    sigma = np.cov(act, rowvar=False)\n",
    "    return mu, sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \"\"\"Numpy implementation of the Frechet Distance.\n",
    "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
    "    and X_2 ~ N(mu_2, C_2) is\n",
    "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
    "            \n",
    "    Stable version by Dougal J. Sutherland.\n",
    "\n",
    "    Params:\n",
    "    -- mu1 : Numpy array containing the activations of the pool_3 layer of the\n",
    "             inception net ( like returned by the function 'get_predictions')\n",
    "             for generated samples.\n",
    "    -- mu2   : The sample mean over activations of the pool_3 layer, precalcualted\n",
    "               on an representive data set.\n",
    "    -- sigma1: The covariance matrix over activations of the pool_3 layer for\n",
    "               generated samples.\n",
    "    -- sigma2: The covariance matrix over activations of the pool_3 layer,\n",
    "               precalcualted on an representive data set.\n",
    "\n",
    "    Returns:\n",
    "    --   : The Frechet Distance.\n",
    "    \"\"\"\n",
    "\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "\n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "\n",
    "    assert mu1.shape == mu2.shape, \"Training and test mean vectors have different lengths\"\n",
    "    assert sigma1.shape == sigma2.shape, \"Training and test covariances have different dimensions\"\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "    # product might be almost singular\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = \"fid calculation produces singular product; adding %s to diagonal of cov estimates\" % eps\n",
    "\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "\n",
    "    # numerical error might give slight imaginary component\n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError(\"Imaginary component {}\".format(m))\n",
    "        covmean = covmean.real\n",
    "\n",
    "    tr_covmean = np.trace(covmean)\n",
    "\n",
    "    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\n",
    "\n",
    "\n",
    "def preprocess_image(im):\n",
    "    \"\"\"Resizes and shifts the dynamic range of image to 0-1\n",
    "    Args:\n",
    "        im: np.array, shape: (H, W, 3), dtype: float32 between 0-1 or np.uint8\n",
    "    Return:\n",
    "        im: torch.tensor, shape: (3, 299, 299), dtype: torch.float32 between 0-1\n",
    "    \"\"\"\n",
    "    assert im.shape[2] == 3\n",
    "    assert len(im.shape) == 3\n",
    "    if im.dtype == np.uint8:\n",
    "        im = im.astype(np.float32) / 255\n",
    "    im = cv2.resize(im, (299, 299))\n",
    "    im = np.rollaxis(im, axis=2)\n",
    "    im = torch.from_numpy(im)\n",
    "    assert im.max() <= 1.0\n",
    "    assert im.min() >= 0.0\n",
    "    assert im.dtype == torch.float32\n",
    "    assert im.shape == (3, 299, 299)\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "# def preprocess_images(images, use_multiprocessing):\n",
    "#     \"\"\"Resizes and shifts the dynamic range of image to 0-1\n",
    "#     Args:\n",
    "#         images: np.array, shape: (N, H, W, 3), dtype: float32 between 0-1 or np.uint8\n",
    "#         use_multiprocessing: If multiprocessing should be used to pre-process the images\n",
    "#     Return:\n",
    "#         final_images: torch.tensor, shape: (N, 3, 299, 299), dtype: torch.float32 between 0-1\n",
    "#     \"\"\"\n",
    "#     if use_multiprocessing:\n",
    "#         with multiprocessing.Pool(multiprocessing.cpu_count()) as pool:\n",
    "#             jobs = []\n",
    "#             for im in images:\n",
    "#                 job = pool.apply_async(preprocess_image, (im,))\n",
    "#                 jobs.append(job)\n",
    "#             final_images = torch.zeros(images.shape[0], 3, 299, 299)\n",
    "#             for idx, job in enumerate(jobs):\n",
    "#                 im = job.get()\n",
    "#                 final_images[idx] = im#job.get()\n",
    "#     else:\n",
    "#         final_images = torch.stack([preprocess_image(im) for im in images], dim=0)\n",
    "#     assert final_images.shape == (images.shape[0], 3, 299, 299)\n",
    "#     assert final_images.max() <= 1.0\n",
    "#     assert final_images.min() >= 0.0\n",
    "#     assert final_images.dtype == torch.float32\n",
    "#     return final_images\n",
    "\n",
    "def preprocess_images(images):\n",
    "    \"\"\"Resizes and shifts the dynamic range of image to 0-1\n",
    "    Args:\n",
    "        images: np.array, shape: (N, H, W, 3), dtype: float32 between 0-1 or np.uint8\n",
    "    Return:\n",
    "        final_images: torch.tensor, shape: (N, 3, 299, 299), dtype: torch.float32 between 0-1\n",
    "    \"\"\"\n",
    "    final_images = torch.stack([preprocess_image(im) for im in images], dim=0)\n",
    "    \n",
    "    assert final_images.shape == (images.shape[0], 3, 299, 299)\n",
    "    assert final_images.max() <= 1.0\n",
    "    assert final_images.min() >= 0.0\n",
    "    assert final_images.dtype == torch.float32\n",
    "    \n",
    "    return final_images\n",
    "\n",
    "def calculate_fid(images1, images2, batch_size):\n",
    "    \"\"\" Calculate FID between images1 and images2\n",
    "    Args:\n",
    "        images1: np.array, shape: (N, H, W, 3), dtype: np.float32 between 0-1 or np.uint8\n",
    "        images2: np.array, shape: (N, H, W, 3), dtype: np.float32 between 0-1 or np.uint8\n",
    "        use_multiprocessing: If multiprocessing should be used to pre-process the images\n",
    "        batch size: batch size used for inception network\n",
    "    Returns:\n",
    "        FID (scalar)\n",
    "    \"\"\"\n",
    "    images1 = preprocess_images(images1)\n",
    "    images2 = preprocess_images(images2)\n",
    "    mu1, sigma1 = calculate_activation_statistics(images1, batch_size)\n",
    "    mu2, sigma2 = calculate_activation_statistics(images2, batch_size)\n",
    "    fid = calculate_frechet_distance(mu1, sigma1, mu2, sigma2)\n",
    "    return fid\n",
    "\n",
    "def resize_and_crop_images(images, image_size=64):\n",
    "    upscale_size = round(image_size * 5 / 4)\n",
    "    final_images = []\n",
    "\n",
    "    for img in images:\n",
    "        # Resize (upscale) the image\n",
    "        resized_img = cv2.resize(img, (upscale_size, upscale_size), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        # Randomly crop a region of the image\n",
    "        x = random.randint(0, max(0, upscale_size - image_size))\n",
    "        y = random.randint(0, max(0, upscale_size - image_size))\n",
    "        cropped_img = resized_img[y:y+image_size, x:x+image_size]\n",
    "\n",
    "        # Append the processed image\n",
    "        final_images.append(cropped_img)\n",
    "\n",
    "    return np.array(final_images)\n",
    "\n",
    "\n",
    "\n",
    "# def load_images(path):\n",
    "#     \"\"\" \n",
    "#     Loads all .jpg images from a given path.\n",
    "#     Warnings: Expects all images to be of the same dtype and shape.\n",
    "#     Args:\n",
    "#         path: relative path to directory\n",
    "#     Returns:\n",
    "#         final_images: np.array of image dtype and shape.\n",
    "#     \"\"\"\n",
    "#     if not os.path.exists(path):\n",
    "#         raise ValueError(f\"Path {path} does not exist\")\n",
    "    \n",
    "#     image_extensions = [\"jpg\", \"png\"]\n",
    "#     image_paths = []\n",
    "#     for ext in image_extensions:\n",
    "#         image_paths.extend(glob.glob(os.path.join(path, f\"*.{ext}\")))\n",
    "\n",
    "\n",
    "\n",
    "#     if not image_paths:\n",
    "#         raise ValueError(f\"No JPG images found in directory {path}\")\n",
    "\n",
    "#     # Initialize an empty list for storing image arrays\n",
    "#     images = []\n",
    "\n",
    "#     for impath in image_paths:\n",
    "#         im = cv2.imread(impath, cv2.IMREAD_COLOR)\n",
    "#         if im is None:\n",
    "#             continue  # Skip if the image can't be read\n",
    "\n",
    "#         im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n",
    "\n",
    "#         # Append to the list if the image is valid\n",
    "#         images.append(im)\n",
    "\n",
    "#     if not images:\n",
    "#         raise ValueError(\"No valid images found in the directory\")\n",
    "\n",
    "#     # Check if all images have the same shape and dtype\n",
    "#     first_image_shape = images[0].shape\n",
    "#     first_image_dtype = images[0].dtype\n",
    "\n",
    "#     if not all(im.shape == first_image_shape and im.dtype == first_image_dtype for im in images):\n",
    "#         raise ValueError(\"Not all images have the same shape and dtype\")\n",
    "\n",
    "#     # Convert list of images to a numpy array\n",
    "#     final_images = np.array(images)\n",
    "\n",
    "#     return final_images\n",
    "\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def load_images(path):\n",
    "    \"\"\" \n",
    "    Loads a maximum of 4000 .png or .jpg images from a given path.\n",
    "    Randomly samples 4000 images if more are available.\n",
    "    Warnings: Expects all images to be of same dtype and shape.\n",
    "    Args:\n",
    "        path: relative path to directory\n",
    "    Returns:\n",
    "        final_images: np.array of image dtype and shape.\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    image_extensions = [\"png\", \"jpg\"]\n",
    "    for ext in image_extensions:\n",
    "        print(\"Looking for images in\", os.path.join(path, f\"*.{ext}\"))\n",
    "        image_paths.extend(glob.glob(os.path.join(path, f\"*.{ext}\")))\n",
    "\n",
    "    # Randomly sample 4000 images if more are available\n",
    "    if len(image_paths) > 1500:\n",
    "        image_paths = random.sample(image_paths, 1500)\n",
    "\n",
    "    if not image_paths:\n",
    "        return np.array([])  # Return an empty array if no images found\n",
    "\n",
    "    first_image = cv2.imread(image_paths[0])\n",
    "    W, H = first_image.shape[:2]\n",
    "    final_images = np.zeros((len(image_paths), H, W, 3), dtype=first_image.dtype)\n",
    "\n",
    "    for idx, impath in enumerate(image_paths):\n",
    "        im = cv2.imread(impath)\n",
    "        im = im[:, :, ::-1] # Convert from BGR to RGB\n",
    "        assert im.dtype == final_images.dtype\n",
    "        final_images[idx] = im\n",
    "\n",
    "    return final_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for images in fid_real_images/*.png\n",
      "Looking for images in fid_real_images/*.jpg\n",
      "Looking for images in single/*.png\n",
      "Looking for images in single/*.jpg\n"
     ]
    }
   ],
   "source": [
    "real_images = load_images('fid_real_images/')\n",
    "fake_images = load_images('single/')\n",
    "real_images = resize_and_crop_images(real_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_value = calculate_fid(real_images, fake_images, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.24366777965821"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fid_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
